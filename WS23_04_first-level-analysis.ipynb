{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTped0ISfxa8tp+Y87AaXL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wy5VuKNX23BF"},"source":["# 06 - First-Level Analysis\n","\n","In last week's notebook we had a look on how to inspect the quality of our data and preprocessing results. Once the data has been preprocessed we can start with the actual statistical analysis. Recall from the lecture that the first step of the analysis takes place on the subject level. That is, each subject's data will be modelled separately. This is also known as **First Level Analysis**. In this notebook, we will learn how to perform such analysis using Python with the help of the Nilearn's GLM submodule.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dPgXTZckUihz"},"source":["#### Questions\n","\n","- How to make statistical inference on a single-subject level?\n","\n","\n","#### Objective\n","\n","- Learn to create statistical maps using a mass univariate approach"]},{"cell_type":"markdown","metadata":{"id":"tw9QdkCaGlsE"},"source":["## Install dependencies and data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RT72Ud80J5vv"},"outputs":[],"source":["!pip install nilearn"]},{"cell_type":"markdown","metadata":{"id":"zuh7wVn1kae0"},"source":["For illustration purposes, we will use a dataset that comes with Nilearn (check out the documentation of the [```datasets```](https://nilearn.github.io/stable/modules/datasets.html) submodule for other available datasets).\n","\n","The dataset includes the functional data of a single subject who completed the tasks described in [Pinel et al. (2007)](https://doi.org/10.1186/1471-2202-8-91). In their study, participants had to perform basic operations such as pressing a button with the left or right hand, reading and listening to short sentences or viewing horizontal and vertical checkerboards."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tg_EqMa2xMg"},"outputs":[],"source":["from nilearn.datasets import func\n","data = func.fetch_localizer_first_level()"]},{"cell_type":"markdown","metadata":{"id":"zFkuYlfgSzKW"},"source":["Below are the files that come with the Nilearn dataset. It compromises a preprocessed functional image (normalized to the MNI template) and a table (```.tsv```) specifying the experimental paradigm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hn3EAxIlJ49U"},"outputs":[],"source":["! ls /root/nilearn_data/localizer_first_level/localizer_first_level"]},{"cell_type":"markdown","metadata":{"id":"kkHmxswlTwur"},"source":["As usual, we first load in the data, check the shape of it and plot the image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxFf92Q9kGus"},"outputs":[],"source":["import nibabel as nib\n","\n","func = nib.load(\"/root/nilearn_data/localizer_first_level/localizer_first_level/sub-12069_task-localizer_space-MNI305.nii.gz\")\n","print(func.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDRHgqt1lojQ"},"outputs":[],"source":["from nilearn import plotting, image\n","\n","plotting.view_img(image.mean_img(func))"]},{"cell_type":"markdown","metadata":{"id":"DrfZjQARnHG0"},"source":["## Defining the experiment"]},{"cell_type":"markdown","metadata":{"id":"z-vRmi2xnYC8"},"source":["The BIDS standard specifies that for each run and for each subject the dataset has to include a file specifying the timing and other properties of events that took place in the respective run - the ```events.tsv``` file (see the [BIDS](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/05-task-events.html) documentation).\n","\n","This file always has to include information regarding the time of the **onset** of the stimulus/condition, and its **duration**. It also includes further information regarding the experimental paradigm (such as **trial type**)\n","\n","Let's have a look at the ```.tsv``` file of our dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB7Kbs4dl2h1"},"outputs":[],"source":["import pandas as pd\n","\n","events = pd.read_table(\"/root/nilearn_data/localizer_first_level/localizer_first_level/sub-12069_task-localizer_events.tsv\")\n","\n","# have a quick look at the data\n","print(events.head(5))\n","print(events.tail(5))"]},{"cell_type":"markdown","metadata":{"id":"g5Ss2td3n8zT"},"source":["The experiment paradigm of our sample datasets includes 10 different conditions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aig5uRk2m2GE"},"outputs":[],"source":["print(events.trial_type.unique())"]},{"cell_type":"markdown","metadata":{"id":"L4KeQwkcoKWi"},"source":["Thanks to the ```.tsv``` file we know when and for how long a given condition takes place in the experiment. This way, we can relate it to the timecourse of our fMRI data - using a general linear model (GLM)!\n"]},{"cell_type":"markdown","metadata":{"id":"3_6Blvzq9ujx"},"source":["### Specifying the Model"]},{"cell_type":"markdown","metadata":{"id":"nz76QmdhY2Ay"},"source":["To predict the voxels timecourses we will specify a general linear model using Nilearn's ```FirstLevelModel``` class:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RI7JG-XJrhFo"},"outputs":[],"source":["from nilearn.glm.first_level import FirstLevelModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3ef_DubrhWU"},"outputs":[],"source":["help(FirstLevelModel)"]},{"cell_type":"markdown","metadata":{"id":"mNQC1RTBZCcE"},"source":["As can be seen in the documentation, we need to provide the repition time (```t_r```) of our functional scans, i.e., the time it takes to scan one volume. This parameter is important as it implies at which point in time we have sampled the voxels responses.\n","\n","Normally in a BIDS dataset, the repition time is specified in a ```.json``` file or included in the header. For this dataset, however, we have to set the repitition time manually (which can be found in Pinel et al. (2007))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJRObA9Po0BX"},"outputs":[],"source":["tr = 2.4"]},{"cell_type":"markdown","metadata":{"id":"KoWmt2kBa-jJ"},"source":["Using the repition time, we can now setup our linear model using the ```FirstLevelModel``` class. In this step, we specify which hemodynamic reponse function (HRF) to use. Here, we will use SPM's default function, but we could also include further time derivatives as regressors (see documentation). Also, we will set a high pass filter (removing signals below that threshold) und use the default drift model (to account for signal drift in the course of the experiment)."]},{"cell_type":"markdown","metadata":{"id":"e2uwHy-892_j"},"source":["### Fitting the model to the data"]},{"cell_type":"markdown","metadata":{"id":"nktDEVYUCsEJ"},"source":["Now that we have specified the model, we can fit our data to it. In this step, we have to provide our events table. This way, the HRF and the timings of the respective conditions are [convolved](https://en.wikipedia.org/wiki/Convolution), resulting in an **ideal time-series** that represents a given voxel's time-series *if* that voxel is responsive to a given condition. Importantly, we are using a mass univariate approach, meaning that we want to model *every* voxel.\n","\n","\n","Note that in this step, we could also include the motion parameters as further regressors (see the confounds parameter of ```FirstLevelModel```) but they are not included in this dataset. For a tutorial where motion parameters have been included check the [References & Resources](#References-&-resources) section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7UC1JPprhnG"},"outputs":[],"source":["first_level = first_level.fit(image.smooth_img(func, 4), events = events)"]},{"cell_type":"markdown","metadata":{"id":"qeRRylLnENse"},"source":["The ```.fit``` method yields both the beta maps (i.e., the regression weight for each predictor) and the design matrix - the statistical model of the data. It contains the ideal time-series for each experimental condition (the predictor variables) as well as some further nuisance regressors (note that the motion parameters would have been included as further predictors).\n","\n","*Note that since the data has not been smoothed during preprocessing, we are apply smoothing with a kernel width of 4 using Nilearn's ```smooth_img``` function, increasing statistical power*\n","\n","We can visualize the design matrix using Nilearn (it's considered to inspect the design matrix before continuing with the analysis):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zFN2piSrhq4"},"outputs":[],"source":["design_matrix = first_level.design_matrices_[0]\n","plotting.plot_design_matrix(design_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPGXTqCgE_-W"},"outputs":[],"source":["print(design_matrix.shape)"]},{"cell_type":"markdown","metadata":{"id":"hVmkanpfe2HS"},"source":["As you can see, the design matrix consists of 16 columns, representing the predictors (10 experimental conditions). The rows indicate the observations, i.e., points in the time-series. Each column consists of the ideal time-series for the respective conditions (go back to the events file and see if it makes sense to you)\n","\n","Let's plot the first column (representing the ideal time-series for the audio computation time-series):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fS9-DSeuEGRv"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(design_matrix[\"audio_computation\"])"]},{"cell_type":"markdown","metadata":{"id":"ZkJVMvtsrISU"},"source":["### Contrasts: comparing experimental conditions\n"]},{"cell_type":"markdown","metadata":{"id":"DBouQtGhiLwY"},"source":["Now, if we want to know which voxels respond more/less/differently to a certain condition than to another condition (or baseline), we have to compute **contrasts** - the difference in beta estimates between conditions.\n","\n","To calculate a contrast, we have to assign weights to each Beta (i.e., each predictor -> design matrix column).\n","\n","In the following we want to check which voxels show more/less activity in the sentence listening condition compared to the baseline. Furthermore, we want to know which voxels are more/less active when pressing a button with the right hand compared to pressing it with the left hand. Finally, we want to compute a contrast for all conditions involving predominantly visual operations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f66vOuqpEyiQ"},"outputs":[],"source":["import numpy as np\n","\n","conditions = {\n","  \"sentence_listening > baseline\":  np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n","  \"right > left\":                   np.array([0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0]),\n","  \"avg_visual\":                     np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n","}"]},{"cell_type":"markdown","metadata":{"id":"p8NGRf6qjSk4"},"source":["We have to assign a weight to each predictor of our design matrix. Thus the array to specify a contrast has a length of 16."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awCmi0aqFhuZ"},"outputs":[],"source":["for id, val in conditions.items():\n","  plotting.plot_contrast_matrix(val, design_matrix=design_matrix)"]},{"cell_type":"markdown","metadata":{"id":"JAGpm_4OZmWe"},"source":["Above, we defined the contrasts manually (i.e., setting the weights ourselves). However, Nilearn also allows us to define simple contrasts with verbal labels (i.e., the design matrix column labels):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBtACLGcZlM-"},"outputs":[],"source":["plotting.plot_contrast_matrix(\"visual_left_hand_button_press - visual_right_hand_button_press\",\n","                              design_matrix=design_matrix)"]},{"cell_type":"markdown","metadata":{"id":"x7I2XtxjF7j3"},"source":["Now that we have defined the contrasts of interest, we can calculate the respective statistics. To investigate statistical significance we will calculate a t-statistic for each voxel and directly convert it into z-scale (we will use the ```conditions``` dictionary we defined earlier but you could also use verbal labels as in the cell above):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OB0CAb1jGMjX"},"outputs":[],"source":["z_map_right_left = first_level.compute_contrast(conditions[\"right > left\"],\n","                                                output_type=\"z_score\")"]},{"cell_type":"markdown","metadata":{"id":"ZAOxYyFirW3f"},"source":["Next, we will display the statistical map on top of the MNI template (this is the default background image of the ```plot_stat_map``` function but we could also use the subject's anatonomy, see the ```bg_img``` parameter). Using the treshold parameter, we also specify that only voxels with an absolute z-value larger than 3 will be displayed (this is for illustration purposes only and no proper statistical inference)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rLli0o8G2Uw"},"outputs":[],"source":["plotting.plot_stat_map(z_map_right_left,\n","                       display_mode=\"z\",\n","                       cut_coords=5,\n","                       threshold = 3.0,\n","                       title=\"right_hand > left_hand\", symmetric_cbar=True)"]},{"cell_type":"markdown","metadata":{"id":"D7W8fHLAG3vj"},"source":["As expected, voxels in the motor/somatosensory cortex of the left hemisphere are more active when pressing a button with the right hand. To recap the theory of contrasts: For right>left contrast, we weigh the Betas of the right and left button press condition with 1 and -1, respectively. Consider for example a voxel in the left M1: Say for this voxel, our model estimated a β of 4 for the right button press condition (i.e., high activity) and a β of 0.1 for the left button press condition (almost no activity): We now weigh these β with our [1, -1] contrast (and 0 for all other conditions). That is: $$ 4*1 + 0.1 * -1 = 3.9  $$   \n","\n","So for this specific voxel in the left M1 we would end up with z-scaled value of 3.9 (i.e., a rather high value -> yellowish dot in the statistical map). This is done for all voxels in the brain (mass univariate approach) and the end result is the statistical map we see above.\n","\n","Importantly, you can also use such contrasts as a **sanity check**: Think about contrasts between conditions in your experiment where you are certain that a specific region shows a increased BOLD response (e.g. activity in the left motor cortex for a right-hand button press or V1 activity for eyes open vs. eyes closed). If the results don't match you expectations, chances are that something went wrong and you would have to double check your preprocessing pipeline.\n"]},{"cell_type":"markdown","metadata":{"id":"L8TBasS-Uih5"},"source":["#### Correcting for multiple comparisons\n","\n","So far we haven't tested the voxels' responses for significance yet. For this, we will import another Nilearn function.\n","\n","A popular way of correcting for multiple comparisons in the context of fMRI data is the [false discovery rate](https://en.wikipedia.org/wiki/False_discovery_rate) which is the expected proportion of false discoveries among detections - again, check the lecture slides or the [Nilearn documentation](https://nilearn.github.io/stable/glm/glm_intro.html#multiple-comparisons) for more information on this. Also, have a look at this nice [notebook on  multiple comparison correction](https://lukas-snoek.com/NI-edu/fMRI-introduction/week_6/MCC.html) created by [Lukas Snoek](https://lukas-snoek.com/) from the University of Amsterdam."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nU52impBH1E2"},"outputs":[],"source":["from nilearn.glm.thresholding import threshold_stats_img\n","_, threshold = threshold_stats_img(z_map_right_left, alpha=.05, height_control='fdr')\n","\n","plotting.plot_stat_map(z_map_right_left,\n","                       display_mode=\"z\",\n","                       cut_coords=5,\n","                       threshold = threshold,\n","                       title=\"right_hand > left_hand (fdr corrected; p < 0.05)\")"]},{"cell_type":"markdown","source":["In line 2, ```_,``` is used to indicate that the first element of the tuple can be discarded.\n","\n","\n"],"metadata":{"id":"9Toxut4t2yMO"}},{"cell_type":"markdown","metadata":{"id":"AGu_JMzfJV_h"},"source":["Now we can apply these steps to all of our defined contrasts using a ```for``` loop:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAIbujEaJeI_"},"outputs":[],"source":["for id, val in conditions.items():\n","\n","  z_map=first_level.compute_contrast(val, output_type=\"z_score\")\n","  _, threshold = threshold_stats_img(z_map, alpha=.05, height_control='fdr')\n","\n","  plotting.plot_stat_map(z_map,\n","                         display_mode=\"z\",\n","                         threshold=threshold,\n","                         cut_coords=range(-20, 70, 10),\n","                         title=id)"]},{"cell_type":"markdown","metadata":{"id":"s7QfyC7aCFfP"},"source":["## Reporting\n","\n","### Automatic reporting using Nilearn\n","\n","Another way of checking the results of the first level analysis is to inspect the significant clusters of voxels. For this job, Nilearn provides the ```get_clusters_table``` function which will list the clusters with the most activity.\n","\n","We will set a treshold of 10, so that only clusters with more than 10 voxels will be included in our table (note that this is an arbitrary value and only done here for illustration purposes).\n","\n","Let's get the cluster statistics for the right hand > left hand contrast:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1tNxCIZZMkU"},"outputs":[],"source":["from nilearn.glm.thresholding import threshold_stats_img\n","from nilearn.reporting import get_clusters_table\n","\n","_, threshold = threshold_stats_img(z_map_right_left, alpha=.05, height_control='fdr')\n","\n","cluster_table = get_clusters_table(z_map_right_left, stat_threshold=threshold,\n","                                   cluster_threshold=10)\n","\n","print(cluster_table)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GuMZk_Aa7drp"},"source":["Another nice feature of Nilearn is the ```make_glm_report``` function which outputs a HTML report for all important aspects of a fitted GLM:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-G-FDik7sBu"},"outputs":[],"source":["from nilearn.reporting import make_glm_report\n","my_glm_report = make_glm_report(first_level, contrasts = conditions,\n","                                height_control = \"fdr\", alpha=.05)\n","\n","my_glm_report"]},{"cell_type":"markdown","metadata":{"id":"B_eYzp1r778i"},"source":["We can inspect the report directly in the notebook or save it as a ```.html``` file using the ```save_as_html()``` method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAkpdVqr9Yax"},"outputs":[],"source":["my_glm_report.save_as_html(\"my_report.html\")"]},{"cell_type":"markdown","metadata":{"id":"W_msRd1SjxD-"},"source":["### Adding region labels\n","\n","Another neat way to automatically generate reports of our results is implemented in the Python package [```atlasreader```](https://github.com/miykael/atlasreader). A cool thing about this tool is that it does not only create nice figures but also adds atlas labels for the cluster peaks.\n","\n","In this notebook, we only want to extract a cluster table containing the respective atlabs label - thus we import the ```get_statmap_info``` (however, do check the Githup repository of atlasreader to refer to the usage of its main function ```create_output```):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZVQwlFu06FU"},"outputs":[],"source":["!pip install atlasreader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlMcvdqins4A"},"outputs":[],"source":["from atlasreader import get_statmap_info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9BgUnXNnw01"},"outputs":[],"source":["cluster_info, peak_info = get_statmap_info(z_map_right_left,\n","                                           cluster_extent = 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4xQqbPipbe4"},"outputs":[],"source":["cluster_info.head(10)"]},{"cell_type":"markdown","metadata":{"id":"d190pZ_J06FV"},"source":["---\n","\n","What we did not cover in this notebook are ways to assess the quality of the model fit. For this, you can inspect residuals of the model as well as extract the predicted time series (you need to set ```minimize_memory``` in when initializing the ```FirstLevelModel``` to ```False``` to do so). See [here](https://nilearn.github.io/dev/auto_examples/04_glm_first_level/plot_predictions_residuals.html#sphx-glr-auto-examples-04-glm-first-level-plot-predictions-residuals-py for a tutorial from the Nilearn user guide"]},{"cell_type":"markdown","metadata":{"id":"9m1q3lx7nKz3"},"source":["## References & Resources\n","\n","**Dataset**\n","\n","Pinel, P., Thirion, B., Meriaux, S. et al. Fast reproducible identification and large-scale databasing of individual functional cognitive networks. *BMC Neuroscience*, *91* (2007). https://doi.org/10.1186/1471-2202-8-91\n","\n","Notter M. P., Gale D., Herholz P., Markello R. D., Notter-Bielser M.-L., & Whitaker K. (2019). AtlasReader: A Python package to generate coordinate tables, region labels, and informative figures from statistical MRI images. *Journal of Open Source Software, 4(34), 1257*, [https://doi.org/10.21105/joss.01257](https://doi.org/10.21105/joss.01257)\n","\n","\n","---\n","\n","\n","**Tutorials covering First-Level analysis in Python:**\n","\n","\n","[Nilearn User Guide on First-Level models](https://nilearn.github.io/stable/glm/first_level_model.html)\n","\n","\n","[Nilearn statistical analysis tutorial](https://github.com/miykael/workshop_pybrain/blob/master/workshop/notebooks/04b_statistical_analyses_MRI.ipynb) & corresponding [Youtube tutorial](https://youtu.be/4FVGn8vodkc?t=15105)\n","- includes both first and second level analysis\n","- covers further concepts, such as inclusion of motion parameters, F-tests or model evaluation\n","\n","[Nilearn documentation: GLM First Level](https://nilearn.github.io/auto_examples/04_glm_first_level/plot_first_level_details.html#sphx-glr-auto-examples-04-glm-first-level-plot-first-level-details-py)\n","- same dataset as in this notebook, but more in-depth\n","- data masking, changing the HRF function, adjusting the drift model\n","\n","Below, there also two more in-depth tutorials covering the First-Level analysis within Python, however, with \"manual\" model specification and fitting (i.e., without the help of Nilearn).\n","\n","https://lukas-snoek.com/NI-edu/fMRI-introduction/week_2/glm_part1_estimation.html\n","\n","https://lukas-snoek.com/NI-edu/fMRI-introduction/week_3/glm_part2_inference.html\n"]}]}
